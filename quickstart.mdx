---
title: "Quickstart"
---

[**Paywalls.ai**](http://Paywalls.ai) is a programmable paywall and usage-based billing proxy for OpenAI-compatible APIs. It enables developers to monetize access to LLM models without modifying the client-side integration logic. The proxy layer intercepts API calls, validates the userâ€™s connection and balance, calculates cost, charges accordingly, and only then forwards the request to the LLM provider.

This system is designed to be **fully compatible with OpenAIâ€™s API specification**, making it drop-in for existing applications using `/v1/chat/completions` or other endpoints.

OpenAPI specification: https://api.paywalls.ai/v1/openapi.yml

### ðŸ”§ Core Components

| Component                                | Description                                                                                                     |
| :--------------------------------------- | :-------------------------------------------------------------------------------------------------------------- |
| **Proxy endpoint** (`/chat/completions`) | Handles LLM chat requests. Enforces paywall rules and proxies to OpenAI-compatible model providers.             |
| **Model registry** (`/models`)           | Lists supported models and their pricing structure (per-token and per-request).                                 |
| **User endpoints** (`/user/*`)           | Provides optional utilities to manage paywall connection, top-up, balance lookup, and manual charging.          |
| **Pricing engine**                       | Calculates usage cost based on configured model rates and request parameters (e.g., tokens, request flat fees). |
| **Metering system**                      | Measures prompt and completion token usage (if token-based billing is enabled).                                 |
| **Connection logic**                     | Ensures only pre-approved users can be charged. If not connected, the API returns a `connect` link.             |
| **Top-up system**                        | When user balance is low, returns a `topup` link instead of charging or forwarding the request.                 |

### ðŸ”„ Request Lifecycle

1. **Client sends request** to `/chat/completions` (OpenAI-compatible).
2. **Authorization checked** via API key in `Authorization: Bearer ...`.
3. **User state verified**:
   - If not connected â†’ return `connected: false` \+ `connect` link.
   - If connected, check balance:
     - Insufficient balance â†’ return `connected: true` \+ `topup` link.
     - Sufficient balance â†’ continue.
4. **Cost computed**:
   - Request fee (if configured).
   - Prompt \+ completion token cost (based on actual usage or estimate).
5. **Charge issued**:
   - Balance deducted.
   - Charge recorded (optional metadata for reconciliation).
6. **Request forwarded** to the real model provider (e.g., OpenAI, OpenRouter, local LLM).
7. **Streaming or full response** returned to client.
8. **Usage record stored** (optional, for dashboards, analytics, or reconciliation).

### ðŸ’¸ Billing Options

You can configure billing logic per paywall or per model:

- **Per-request fee**: A flat cost applied for every request.
- **Per-token billing**: Based on measured prompt and completion token usage.
- **Subscriptions or memberships**: Can be handled externally; proxy checks for plan validity via your own logic.
- **Manual charges**: Use `/user/charge` to deduct custom fees.

### ðŸ§° Developer Usage

- **Integrate once**: No changes needed on the frontend. Just point requests to `https://api.paywalls.ai/v1/chat/completions` instead of `api.openai.com`.
- **Authenticate your app**: Use your **developer API key** in the `Authorization: Bearer YOUR_API_KEY` header. This key identifies your paywall configuration.
- **Identify the user**:
  - For **chat completions**, include the userâ€™s ID either:
    - As a `user` property in the JSON request body (OpenAI-compatible), or
    - As an `X-Paywall-User` header (useful for non-OpenAI clients or middleware injection).
  - For **all other endpoints** (e.g., `/user/charge`, `/user/balance`), pass the user ID in the required request field (typically in the JSON body).
- **Track usage**: Monitor charges, user sessions, and connection status through API responses or optional integrations (e.g., webhooksâ€”coming soon).
- **Custom workflows**: Use `/user/connect`, `/user/topup`, and `/user/charge` to build your own onboarding, billing, or subscription logic.

### âœ… Benefits

- Drop-in compatible with OpenAI API.
- Eliminates the need to build billing, metering, or access control logic.
- Works with any frontend or backend stack.
- Enables new monetization models like pay-per-message, microtransactions, and token quotas.